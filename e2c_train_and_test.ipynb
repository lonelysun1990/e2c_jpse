{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interactive workflow for E2C training and evaluation. Note that this specific case may generate results that are distinct from the paper.\n",
    "\n",
    "During the training process (while the last cell is running), you can monitor the training status with Tensorboard. Make sure `tensorboard` is installed properly. To install `tensorboard`:  \n",
    "`pip install tensorboard`  \n",
    "\n",
    "All the data used for `tensorboard` are stored in `logs/` directory. If you do not have `logs/` directory in your cloned repo, please create one. To turn on `tensorboard`:  \n",
    "`tensorboard --logdir=logs --port=5678` (`--port` is necesary for port-forwarding)\n",
    "\n",
    "\n",
    "Zhaoyang Larry Jin  \n",
    "Stanford University  \n",
    "zjin@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has three sections: `0. E2C setup`, `1. E2C Training`, and `2. E2C Eval`\n",
    "\n",
    "A typical workflow is `sec 0` -> `sec 1` -> `sec 2`.\n",
    "\n",
    "If you have already run `sec 1` before and have saved the model weights, you can do `sec 0` -> `sec 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: E2C setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load libaraies and config hardware (gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:00:46.883031Z",
     "start_time": "2021-07-06T00:00:39.994532Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from e2c import E2C\n",
    "from loss import CustomizedLoss\n",
    "from ROMWithE2C import ROMWithE2C\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'jet'\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:00:55.119591Z",
     "start_time": "2021-07-06T00:00:55.112251Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:00:56.542655Z",
     "start_time": "2021-07-06T00:00:56.536346Z"
    }
   },
   "outputs": [],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "for device in devices:\n",
    "    print(device.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:00:58.397789Z",
     "start_time": "2021-07-06T00:00:58.392245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "    # you can either do with or without '/device:'\n",
    "else:\n",
    "    device = '/device:CPU:0'\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Specify params and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:01:01.292597Z",
     "start_time": "2021-07-06T00:01:01.287476Z"
    }
   },
   "outputs": [],
   "source": [
    "################### case specification ######################\n",
    "\n",
    "data_dir = '../data/'\n",
    "output_dir = './saved_models/'\n",
    "\n",
    "case_name = '9w_ms_bhp_rate'\n",
    "case_suffix = '_fix_wl_rel_8'\n",
    "train_suffix = '_with_p'\n",
    "model_suffix = '_flux_loss'\n",
    "\n",
    "n_train_run = 300\n",
    "n_eval_run = 100\n",
    "num_t = 20 \n",
    "dt = 100\n",
    "n_train_step = n_train_run * num_t\n",
    "n_eval_step = n_eval_run * num_t\n",
    "\n",
    "\n",
    "train_file = case_name + '_e2c_train' + case_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat' %(n_train_step, dt, num_t, n_train_run)\n",
    "eval_file = case_name + '_e2c_eval' + case_suffix + train_suffix +'_n%d_dt%dday_nt%d_nrun%d.mat' %(n_eval_step, dt, num_t, n_eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:01:02.531485Z",
     "start_time": "2021-07-06T00:01:02.527827Z"
    }
   },
   "outputs": [],
   "source": [
    "#################### model specification ##################\n",
    "epoch = 10\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "latent_dim = 50\n",
    "\n",
    "u_dim = 9*2 # control dimension, gaussian 9 wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:05:59.539528Z",
     "start_time": "2021-07-06T00:05:59.535291Z"
    }
   },
   "outputs": [],
   "source": [
    "num_train = 6000\n",
    "num_eval = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:01:03.227748Z",
     "start_time": "2021-07-06T00:01:03.223869Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (60, 60, 2)\n",
    "perm_shape = (60, 60, 1)\n",
    "prod_loc_shape = (5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:06:01.082221Z",
     "start_time": "2021-07-06T00:06:01.073851Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_file = output_dir + 'e2c_encoder_dt_'+case_name+case_suffix+train_suffix+model_suffix+'_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch)\n",
    "decoder_file = output_dir + 'e2c_decoder_dt_'+case_name+case_suffix+train_suffix+model_suffix+'_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch)\n",
    "transition_file = output_dir + 'e2c_transition_dt_'+case_name+case_suffix+train_suffix+model_suffix+'_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch)\n",
    "\n",
    "print(\"encoder_file:\", encoder_file)\n",
    "print(\"decoder_file:\", decoder_file)\n",
    "print(\"transition_file:\", transition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Construct E2C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:06:07.926790Z",
     "start_time": "2021-07-06T00:06:07.137958Z"
    }
   },
   "outputs": [],
   "source": [
    "my_rom = ROMWithE2C(latent_dim, \n",
    "                    u_dim, \n",
    "                    input_shape, \n",
    "                    perm_shape, \n",
    "                    prod_loc_shape, \n",
    "                    learning_rate,\n",
    "                    sigma=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: E2C Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:35:39.380469Z",
     "start_time": "2021-07-05T23:35:37.288856Z"
    }
   },
   "source": [
    "## Load state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:07:20.524913Z",
     "start_time": "2021-07-06T00:07:17.630464Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_r = h5py.File(data_dir + train_file, 'r')\n",
    "state_t_train = np.array(hf_r.get('state_t'))\n",
    "state_t1_train = np.array(hf_r.get('state_t1'))\n",
    "bhp_train = np.array(hf_r.get('bhp'))\n",
    "dt_train = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "assert num_train == state_t_train.shape[0], \"num_train not match!\"\n",
    "# dt_train = np.ones((num_train,1)) # dt=20days, normalized to 1\n",
    "\n",
    "hf_r = h5py.File(data_dir + eval_file, 'r')\n",
    "state_t_eval = np.array(hf_r.get('state_t'))\n",
    "state_t1_eval = np.array(hf_r.get('state_t1'))\n",
    "bhp_eval = np.array(hf_r.get('bhp'))\n",
    "dt_eval = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "print(\"state_t_eval.shape: \", state_t_eval.shape)\n",
    "print(\"state_t1_eval.shape: \", state_t1_eval.shape)\n",
    "print(\"bhp_eval.shape: \", bhp_eval.shape)\n",
    "print(\"dt_eval.shape: \", dt_eval.shape)\n",
    "\n",
    "\n",
    "assert num_eval == state_t_eval.shape[0], \"num_eval not match!\"\n",
    "# dt_eval = np.ones((num_eval, 1)) # dt=20days, normalized to 1\n",
    "\n",
    "num_batch = int(num_train/batch_size)\n",
    "print(\"num_batch: \", num_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load permeability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:08:01.411931Z",
     "start_time": "2021-07-06T00:08:01.259475Z"
    }
   },
   "outputs": [],
   "source": [
    "m = np.loadtxt(data_dir + \"template/logk1.dat\") # Gaussian\n",
    "\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('m shape is ', m.shape)\n",
    "\n",
    "m_eval = np.repeat(np.expand_dims(m, axis = 0), state_t_eval.shape[0], axis = 0)\n",
    "print(\"m_eval shape is \", m_eval.shape)\n",
    "\n",
    "m = np.repeat(np.expand_dims(m,axis = 0), state_t_train.shape[0], axis = 0)\n",
    "print(\"m shape is \", m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load well location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:08:02.868100Z",
     "start_time": "2021-07-06T00:08:02.858914Z"
    }
   },
   "outputs": [],
   "source": [
    "well_loc_file = data_dir + 'template/well_loc00.dat'\n",
    "\n",
    "well_loc = np.loadtxt(well_loc_file).astype(int)\n",
    "num_prod = well_loc[0,0]\n",
    "num_inj = well_loc[0,1]\n",
    "num_well = num_prod+num_inj\n",
    "print(num_inj, num_prod)\n",
    "\n",
    "prod_loc = well_loc[1:num_prod+1,:]\n",
    "print(\"prod_loc:\\n{}\".format(prod_loc))\n",
    "print(prod_loc.shape)\n",
    "\n",
    "print('prod_loc shape is ', prod_loc.shape)\n",
    "# prod_loc_tf = tf.placeholder(tf.int32, shape=(num_prod,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:08:04.454563Z",
     "start_time": "2021-07-06T00:08:04.443330Z"
    }
   },
   "outputs": [],
   "source": [
    "## used to generate log directory\n",
    "currentDT = datetime.now()\n",
    "current_time = str(currentDT).replace(\" \", \"-\")[:-10]\n",
    "print(current_time)\n",
    "summary_writer = tf.summary.create_file_writer('logs/' + case_name + case_suffix + '_ep' + str(epoch) + '_tr' + str(n_train_run) + '_' + current_time)\n",
    "\n",
    "# @tf.function\n",
    "def write_summary(value, tag, writer, global_step):\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(tag, value, step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T00:09:51.251895Z",
     "start_time": "2021-07-06T00:08:06.787990Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    for e in range(epoch):\n",
    "        for ib in range(num_batch):\n",
    "            ind0 = ib * batch_size\n",
    "\n",
    "            state_t_batch  = state_t_train[ind0:ind0+batch_size, ...]\n",
    "            state_t1_batch = state_t1_train[ind0:ind0 + batch_size, ...]\n",
    "            bhp_batch      = bhp_train[ind0:ind0 + batch_size, ...]\n",
    "            m_batch        = m[ind0:ind0 + batch_size, ...]\n",
    "            dt_batch       = dt_train[ind0:ind0 + batch_size, ...]\n",
    "\n",
    "            inputs = (state_t_batch, bhp_batch, dt_batch, m_batch, prod_loc)\n",
    "            labels = state_t1_batch\n",
    "\n",
    "            my_rom.update(inputs, labels)\n",
    "\n",
    "            n_itr = e * num_train + ib * batch_size + batch_size\n",
    "            write_summary(my_rom.train_loss.result(), 'train/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "            write_summary(my_rom.train_reconstruction_loss.result(), 'train/reconstruction_loss', summary_writer, n_itr) # log for tensorboard\n",
    "            write_summary(my_rom.train_flux_loss.result(), 'train/flux_loss', summary_writer, n_itr) # log for tensorboard\n",
    "            write_summary(my_rom.train_well_loss.result(), 'train/well_loss', summary_writer, n_itr) # log for tensorboard\n",
    "            summary_writer.flush()\n",
    "\n",
    "            if ib % 50 == 0:\n",
    "                print('Epoch %d/%d, Batch %d/%d, Loss %f,' % (e+1, epoch, ib+1, num_batch, my_rom.train_loss.result()))\n",
    "                test_inputs = (state_t_eval, bhp_eval, dt_eval, m_eval, prod_loc)\n",
    "                test_labels = state_t1_eval\n",
    "                my_rom.evaluate(test_inputs, test_labels)\n",
    "\n",
    "                write_summary(my_rom.test_loss.result(), 'eval/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "                summary_writer.flush()\n",
    "\n",
    "        print('====================================================')\n",
    "        print('\\n')\n",
    "        print('Epoch %d/%d, Train loss %f, Eval loss %f' % (e + 1, epoch, my_rom.train_loss.result(), my_rom.test_loss.result()))\n",
    "        print('\\n')\n",
    "        print('====================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rom.model.saveWeightsToFile(encoder_file, decoder_file, transition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seciton 2: E2C Test (Eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ROM with E2C model (if you did not run section 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:07.297215Z",
     "start_time": "2021-07-05T23:36:07.099936Z"
    }
   },
   "outputs": [],
   "source": [
    "my_rom.model.loadWeightsFromFile(encoder_file, decoder_file, transition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:04.698686Z",
     "start_time": "2021-07-05T23:36:04.692545Z"
    }
   },
   "outputs": [],
   "source": [
    "target_suffix = '_fix_wl_rel_8' # the dataset being evaluated here\n",
    "eval_file = case_name + '_e2c_eval' + target_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat'%(n_eval_step, dt, num_t, n_eval_run)\n",
    "\n",
    "state_file = case_name + '_train_n_400_full'\n",
    "ctrl_file = case_name + '_norm_bhps_n_400'\n",
    "\n",
    "state_data = state_file + target_suffix + '.mat'\n",
    "ctrl_data = ctrl_file + target_suffix + '.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:38.993517Z",
     "start_time": "2021-07-05T23:36:08.795395Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_r = h5py.File(data_dir + state_data, 'r')\n",
    "sat = np.array(hf_r.get('sat'))\n",
    "pres = np.array(hf_r.get('pres'))\n",
    "hf_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.110921Z",
     "start_time": "2021-07-05T23:36:39.020330Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_r = h5py.File(data_dir + ctrl_data, 'r')\n",
    "bhp0 = np.array(hf_r.get('bhp'))\n",
    "rate0 = np.array(hf_r.get('rate'))\n",
    "hf_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.156077Z",
     "start_time": "2021-07-05T23:36:39.125274Z"
    }
   },
   "outputs": [],
   "source": [
    "bhp = np.concatenate((bhp0,rate0),axis=1)\n",
    "print(bhp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.236339Z",
     "start_time": "2021-07-05T23:36:39.218808Z"
    }
   },
   "outputs": [],
   "source": [
    "sat = sat.T.reshape((400, 201, 3600))\n",
    "pres = pres.T.reshape((400, 201, 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.255076Z",
     "start_time": "2021-07-05T23:36:39.244711Z"
    }
   },
   "outputs": [],
   "source": [
    "test_case0 = np.zeros((25,4))\n",
    "a = np.array(range(75,400,100))[np.newaxis,:]\n",
    "b = np.array(range(25))[:,np.newaxis]\n",
    "\n",
    "test_case = (test_case0 + a + b).T.reshape(100)\n",
    "test_case = np.array(test_case).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.293653Z",
     "start_time": "2021-07-05T23:36:39.259577Z"
    }
   },
   "outputs": [],
   "source": [
    "m = np.loadtxt(data_dir + \"template/logk1.dat\") # Gaussian\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('perm shape is ', m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick 4 representative test cases to visualize  \n",
    "Note here we have 100 test cases. In the E2C sequential workflow, prediction are done for all of them. However, to keep the notebook clean and short, we will only visualize a subset of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.308205Z",
     "start_time": "2021-07-05T23:36:39.301174Z"
    }
   },
   "outputs": [],
   "source": [
    "ind_case = np.array([10, 25, 77, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.343088Z",
     "start_time": "2021-07-05T23:36:39.334180Z"
    }
   },
   "outputs": [],
   "source": [
    "num_case = test_case.shape[0] # 4\n",
    "num_tstep = 20\n",
    "sat_pred = np.zeros((num_case, num_tstep, 60, 60, 1))\n",
    "pres_pred = np.zeros((num_case, num_tstep, 60, 60, 1))\n",
    "\n",
    "num_prod = 5\n",
    "num_inj = 4\n",
    "num_well = num_prod + num_inj\n",
    "\n",
    "num_all_case = 400\n",
    "num_ctrl = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify timesteps, time intervals, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the input data  \n",
    "To a proper format, so that it can be easily consumed by E2C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.363166Z",
     "start_time": "2021-07-05T23:36:39.353172Z"
    }
   },
   "outputs": [],
   "source": [
    "t_steps = np.arange(0,200,200//num_tstep)\n",
    "\n",
    "dt = 10\n",
    "t_steps1 = (t_steps + dt).astype(int)\n",
    "\n",
    "indt_del = t_steps1 - t_steps\n",
    "indt_del = indt_del / max(indt_del)\n",
    "\n",
    "tmp = np.array(range(num_tstep)) - 1\n",
    "tmp1 = np.array(range(num_tstep))\n",
    "tmp[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:39.440911Z",
     "start_time": "2021-07-05T23:36:39.371372Z"
    }
   },
   "outputs": [],
   "source": [
    "bhp_b0 = bhp.reshape(num_all_case, num_well, num_ctrl)\n",
    "bhp_b1 = np.repeat(bhp_b0[..., np.newaxis], num_tstep // num_ctrl, axis=3)\n",
    "assert num_tstep // num_ctrl * num_ctrl == num_tstep, \"no exaxt division num_step = %d, num_ctrl=%d\"%(num_tstep, num_ctrl)\n",
    "\n",
    "bhp_b2 = bhp_b1.reshape(num_all_case, num_well, num_tstep)\n",
    "\n",
    "bhp_tt = bhp_b2[:,:, tmp]\n",
    "bhp_tt1 = bhp_b2[:,:, tmp1]\n",
    "\n",
    "bhp_tt0 = np.concatenate((bhp_tt, bhp_tt1), axis=1)\n",
    "bhp_t = np.swapaxes(bhp_tt0,1,2)\n",
    "\n",
    "bhp_seq = bhp_t[test_case, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:53.943385Z",
     "start_time": "2021-07-05T23:36:39.454932Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_t_seq = sat[test_case, 0, :].reshape((num_case, 60, 60, 1)) # 4 here is the 4th timestep, t = 8\n",
    "pres_t_seq = pres[test_case, 0, :].reshape((num_case, 60, 60, 1))\n",
    "\n",
    "state_t_seq = np.concatenate((sat_t_seq, pres_t_seq),axis=3)\n",
    "state_pred = np.concatenate((sat_pred, pres_pred),axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:53.959127Z",
     "start_time": "2021-07-05T23:36:53.946945Z"
    }
   },
   "outputs": [],
   "source": [
    "m_t_seq = np.repeat(np.expand_dims(m, axis = 0), state_t_seq.shape[0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:36:53.968186Z",
     "start_time": "2021-07-05T23:36:53.962553Z"
    }
   },
   "outputs": [],
   "source": [
    "prod_loc_t_seq = np.repeat(np.expand_dims(prod_loc, axis = 0), state_t_seq.shape[0], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E2C sequential workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:38:09.813074Z",
     "start_time": "2021-07-05T23:36:53.997581Z"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "for i_tstep in range(num_tstep):\n",
    "    state_pred[:, i_tstep, ...] = state_t_seq.copy()\n",
    "    dt_seq = np.ones((num_case,1)) * indt_del[i_tstep]\n",
    "    inputs = (state_t_seq, bhp_seq[:,i_tstep,:], dt_seq, m_t_seq, prod_loc_t_seq)\n",
    "    state_t1_seq = my_rom.predict(inputs)\n",
    "    state_t_seq = state_t1_seq.copy()\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(\"Time for sequential process: %f\" %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:39:16.513755Z",
     "start_time": "2021-07-05T23:39:02.333722Z"
    }
   },
   "outputs": [],
   "source": [
    "# sat_seq_true = sat[test_case[ind_case], ...]\n",
    "sat_seq_true = sat[test_case, ...]\n",
    "sat_seq_true = sat_seq_true[:, list(np.arange(0,200,10)), :]\n",
    "\n",
    "# pres_seq_true = pres[test_case[ind_case], ...]\n",
    "pres_seq_true = pres[test_case, ...]\n",
    "pres_seq_true = pres_seq_true[:, list(np.arange(0,200,10)), :]\n",
    "state_seq_true = np.zeros((len(test_case),20,3600,2))\n",
    "state_seq_true[:,:,:,0] = sat_seq_true\n",
    "state_seq_true[:,:,:,1] = pres_seq_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:39:18.513151Z",
     "start_time": "2021-07-05T23:39:18.501488Z"
    }
   },
   "outputs": [],
   "source": [
    "s_max = 1\n",
    "s_min = 0\n",
    "s_diff = s_max - s_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:39:19.166513Z",
     "start_time": "2021-07-05T23:39:19.005516Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_pred_plot = state_pred[:, :, :, :, 0] * s_diff + s_min\n",
    "state_pred[:, :, :, :, 0] = state_pred[:, :, :, :, 0] * s_diff + s_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:39:28.819307Z",
     "start_time": "2021-07-05T23:39:20.569435Z"
    }
   },
   "outputs": [],
   "source": [
    "divide = 2\n",
    "for k in range(4):\n",
    "    print(\"Case num: %d\"%ind_case[k])\n",
    "    plt.figure(figsize=(16,5))\n",
    "    for i_tstep in range(len(t_steps)//divide):\n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1)\n",
    "        plt.imshow(sat_pred_plot[ind_case[k], i_tstep*divide, :,:])\n",
    "        plt.title('t=%d'%(t_steps[i_tstep*divide]*dt))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.clim([0.1, 0.7])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046) \n",
    "            \n",
    "        \n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1+num_tstep//divide)\n",
    "        plt.imshow(state_seq_true[ind_case[k], i_tstep*divide, :, 0].reshape((60,60)))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.clim([0.1, 0.7])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046)         \n",
    "        \n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1+2*num_tstep//divide)\n",
    "        plt.imshow(np.fabs(state_seq_true[ind_case[k], i_tstep*divide, :, 0].reshape((60,60)) - sat_pred_plot[ind_case[k], i_tstep*divide, :,:]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.clim([0, 0.15])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:29:33.676346Z",
     "start_time": "2021-07-05T23:29:33.670829Z"
    }
   },
   "outputs": [],
   "source": [
    "p_max = 425\n",
    "p_min = 250\n",
    "p_diff = p_max - p_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:29:34.097171Z",
     "start_time": "2021-07-05T23:29:33.905429Z"
    }
   },
   "outputs": [],
   "source": [
    "state_pred_plot = state_pred[:, :, :, :, 1] * p_diff + p_min\n",
    "state_seq_true_plot = state_seq_true[:, :, :, 1] * p_diff + p_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T23:29:41.409021Z",
     "start_time": "2021-07-05T23:29:34.574345Z"
    }
   },
   "outputs": [],
   "source": [
    "divide = 2\n",
    "for k in range(4):\n",
    "    print(\"Case num: %d\"%ind_case[k])\n",
    "    plt.figure(figsize=(16,5))\n",
    "    for i_tstep in range(len(t_steps)//divide):\n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1)\n",
    "        plt.imshow(state_pred_plot[ind_case[k], i_tstep*divide, :, :])\n",
    "        plt.title('t=%d'%(t_steps[i_tstep*divide]*dt))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "#         plt.clim([4150, 4650])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046) \n",
    "            \n",
    "        \n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1+num_tstep//divide)\n",
    "        plt.imshow(state_seq_true_plot[ind_case[k], i_tstep*divide, :].reshape((60,60)))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "#         plt.clim([4150, 4650])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046)         \n",
    "        \n",
    "        plt.subplot(3, num_tstep//divide, i_tstep+1+2*num_tstep//divide)\n",
    "        plt.imshow(np.fabs(state_seq_true_plot[ind_case[k], i_tstep*divide, :].reshape((60,60)) - state_pred_plot[ind_case[k], i_tstep*divide, :,:]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "#         plt.clim([0, 0.02])\n",
    "        if i_tstep == 9:\n",
    "            plt.colorbar(fraction=0.046) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL]",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
